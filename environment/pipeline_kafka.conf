##############################################
# Global bits of config in the default section
#
[default]

#
# ID is used to identify pipeline to third parties, e.g. fluentd tag
# and prometheus instance
id = pipeline

#
# Prometheus setup to export metrics about the pipeline. This is where
# prometheus will come and scrape metrics from. This configuration is
# only pertinent if you plan to monitor the pipeline itself using
# prometheus - and can be ignored. You can to comment out
# metamonitoring_prometheus_resource to stop from serving metrics.  If
# you do wish to monitor pipeline, tools/monitor/run.sh will fire
# prometheus and grafana with some prebuilt dashboards (requires
# docker). Point a browser at localhost:3000 as per
# https://github.com/grafana/grafana#running
#
metamonitoring_prometheus_resource = /metrics
metamonitoring_prometheus_server = :8989

# fluentd setup allows pipeline to export its logs in support of
# manageability at scale. Tag used is derived from ID
#
# fluentd = localhost:24224

# base_path_xlation
# This option points at a file which contains JSON base path mappings
# of base_path. This is typically used to morph PDT to MDT paths on
# input side where it makes sense (i.e. gpb and gpb codecs). Not
# applied to SNMP since base path is derived from snmp spec. (JSON is
# missing because today Telemetry header it not assumed for JSON
# inputs. This may change in the future.
# base_path_xlation=bpx.json

#
# Section start with a section name. The only constraint for section
# names is that 'default' is not used. You can have as many sections
# as you wish, as long as each section has a unique name.

##############################################
# Example of a TCP dialout (router connects to pipeline over TCP)
#
[testbed]
stage = xport_input
#
# Module type, the type dictates the rest of the options in the section.
# TCP can only be used as xport_iinput currently. UDP works similarly.
#
type = tcp
#
# Supported encapsulation is 'st' for streaming telemetry header. This
# is the header used to carry streaming telemetry payload in TCP and UDP.
#
encap = st
#
# TCP option dictating which binding to listen on. Can be a host name
# or address and port, or just port.
#
listen = :5432
#
# To enable dumping data as it is rxed, uncomment the following, and
# run with --debug option.
#
# logdata = on
#
# It is also possible to turn on TCP keepalives. Setting keepalive to
# 0 (default) stops pipeline from explicitly turning on
# keepalives. Otherwise, keepalive is turned on with period.  TCP
# Keepalives do NOT need any special explicit configuration or support
# from the other end.
#
# keepalive_seconds = 0


##############################################
# Example of a kafka output stage: used to publish content to kafka
#
[mykafka]
stage = xport_output
#
# Module type: kafka publisher is only supported in xport_output stage currently.
#
type = kafka
#
# Kafka specific key option. This is an optional setting and is
# specific to a weird requirement of our home grown consumer on kafka
# bus.
#
# key = id
#
# Encoding: gpb, gpbkv, json or json_events
#
encoding = json
#
# Kafka specific option. The brokers for the kafka bus
#
brokers = 192.168.10.3:32768
#
# Kafka specific option. The topic to publish against.
#
topic = telemetry
#
# Optional: It is also possible to specify a dynamic derivation of
# topic to send on. This mechanism is a text template applied to the
# metadata of the message; currently a structure containing the Path
# (encoding path) and Identifier (router name) fields. In future the
# whole message body may be exposed. In the case where the template
# fails to extract and return a string, the message will be published
# on 'topic' above.
#
# The example extract encoding path:
#
# topic_metadata_template = topic_template_testb.txt
#
# The option required_acks can be used to influence whether the
# producer waits for acknowledgments from just the local broker
# (i.e. the broker to which the message is published), or for a
# consensus commit from replicas. The options are "local" and "commit"
# respectively. By default, we default to "none".
#
# required_acks = none
#
# The optional buffered channel depth used to accommodate transient
# producer/consumer throughput.
#
# datachanneldepth = 1000
#
# To enable dumping data as it is rxed, uncomment the following, and
# run with --debug option.
#
# logdata = on

